{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNt9rkgPd1XiQm/kAvUeqLv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanskarGithub07/Fine-Tuning-LLMs-for-Spam-Detection-and-Emotion-Classification/blob/main/LLM_Project_2_Sanskar_Sugandhi_2210110898.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise-1"
      ],
      "metadata": {
        "id": "pMs2UvDPbhK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "Re3W-0-LNcQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U fsspec datasets"
      ],
      "metadata": {
        "id": "iyMVSIRQNYSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken transformers datasets"
      ],
      "metadata": {
        "id": "BIDpYZ2gt76N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "    if data_file_path.exists():\n",
        "        print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "        return\n",
        "\n",
        "    # Downloading the file\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        with open(zip_path, \"wb\") as out_file:\n",
        "            out_file.write(response.read())\n",
        "\n",
        "    # Unzipping the file\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(extracted_path)\n",
        "\n",
        "    # Add .tsv file extension\n",
        "    original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "    os.rename(original_file_path, data_file_path)\n",
        "    print(f\"File downloaded and saved as {data_file_path}\")\n",
        "\n",
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ],
      "metadata": {
        "id": "loP3yvFFuH0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and explore the dataset\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
        "df"
      ],
      "metadata": {
        "id": "wqRnwaOsujyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "id": "Z8p_U7-8urFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a balanced dataset\n",
        "def create_balanced_dataset(df):\n",
        "    # Count the instances of \"spam\"\n",
        "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
        "\n",
        "    # Randomly sample \"ham\" instances to match the number of \"spam\" instances\n",
        "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
        "\n",
        "    # Combine ham \"subset\" with \"spam\"\n",
        "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())\n",
        "\n",
        "# Map labels to integers\n",
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
        "balanced_df"
      ],
      "metadata": {
        "id": "n6dy4joUuORg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "def random_split(df, train_frac, validation_frac):\n",
        "    # Shuffle the entire DataFrame\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split indices\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "    # Split the DataFrame\n",
        "    train_df = df[:train_end]\n",
        "    validation_df = df[train_end:validation_end]\n",
        "    test_df = df[validation_end:]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
        "# Save the split datasets\n",
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ],
      "metadata": {
        "id": "9zkJxWnLuYbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SmolLM2 tokenizer from HuggingFace\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the SmolLM2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
        "\n",
        "# If tokenizer doesn't have a padding token set, use the EOS token as padding token. Crucial when batching inputs of different lengths. Pad shorter sequences to match the length of longer sequence\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "m1sjcPruuw3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text, add_special_tokens=True)\n",
        "            for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length() #Automatically calculate the max length of any tokenized sequence\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "            # Truncate sequences if they are longer than max_length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        # Pad sequences to the longest sequence\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [tokenizer.pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length"
      ],
      "metadata": {
        "id": "O6jAdl8eu7vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f\"Maximum sequence length in training set: {train_dataset.max_length}\")\n",
        "\n",
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "0btCSqqevCov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "batch_size = 8\n",
        "num_workers = 0\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "id": "hBphWIn9vGgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "for input_batch, target_batch in train_loader:\n",
        "    break\n",
        "\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "print(\"Label batch dimensions:\", target_batch.shape)\n",
        "\n",
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ],
      "metadata": {
        "id": "PmiQxA5XvKKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Load SmolLM2-135M model\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Create a model class that wraps the pretrained model and adds a classification head\n",
        "class SmolLM2Classifier(torch.nn.Module):\n",
        "    def __init__(self, base_model, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.config = base_model.config\n",
        "\n",
        "        # Freeze all parameters (gradients are not calculated for those params)\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Get embedding dimension\n",
        "        self.hidden_size = self.config.hidden_size\n",
        "\n",
        "        # Add a classification head\n",
        "        self.classifier = torch.nn.Linear(self.hidden_size, num_classes)\n",
        "\n",
        "        # Unfreeze the last transformer block and layer norm\n",
        "        for param in self.base_model.model.layers[-1].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        for param in self.base_model.model.norm.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Get the output from the base model\n",
        "        outputs = self.base_model(input_ids, output_hidden_states=True)\n",
        "\n",
        "        # Get the last hidden state\n",
        "        last_hidden_state = outputs.hidden_states[-1]\n",
        "\n",
        "        # Apply the classifier to get logits\n",
        "        logits = self.classifier(last_hidden_state)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "WqVgjicmvN2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the classification model\n",
        "model = SmolLM2Classifier(base_model)\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "VrZEPPjevUkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return correct_predictions / num_examples\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "CQT0Vc55vgom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check initial performance\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(f\"Initial training loss: {train_loss:.3f}\")\n",
        "print(f\"Initial validation loss: {val_loss:.3f}\")\n",
        "print(f\"Initial test loss: {test_loss:.3f}\")\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
        "\n",
        "print(f\"Initial training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Initial validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Initial test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "XwlZUALivl1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                            eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and examples seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0] # Track examples instead of tokens\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ],
      "metadata": {
        "id": "_5pe3Bezvpl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Create optimizer - note we're only optimizing the classifier parameters and unfrozen layers\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=5e-5,\n",
        "    weight_decay=0.1 #Regularization term penalizes large weights prevents overfitting\n",
        ")\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "Co1oRK9FvzRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    # Create a second x-axis for examples seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(f\"smollm2_{label}-plot.pdf\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yeeta6tYv4B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "1a76ZXoLwBSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
      ],
      "metadata": {
        "id": "o4RQf8Igv-lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate final performance on full datasets\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Final training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Final validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Final test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "AAZu2ygVwG1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_text(text, model, tokenizer, device, max_length=None):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "    # Truncate if needed\n",
        "    if max_length is not None:\n",
        "        input_ids = input_ids[:max_length]\n",
        "\n",
        "    # Pad if needed\n",
        "    if max_length is not None and len(input_ids) < max_length:\n",
        "        input_ids += [tokenizer.pad_token_id] * (max_length - len(input_ids))\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
        "\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Return the classified result\n",
        "    return \"spam\" if predicted_label == 1 else \"not spam\""
      ],
      "metadata": {
        "id": "wrc3dZ7YwLLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_examples = [\n",
        "    \"You are a winner you have been specially selected to receive $1000 cash or a $2000 award.\",\n",
        "    \"Hey, just wanted to check if we're still on for dinner tonight? Let me know!\",\n",
        "    \"URGENT! You have won a 1-week FREE membership in our Â£100,000 Prize Jackpot! Text the word: CLAIM to 81010\",\n",
        "    \"I'll be there in about 15 minutes. See you soon.\",\n",
        "    \"Congratulations! You've been selected for a free iPhone 13. Click here to claim now!\",\n",
        "    \"Can you pick up some milk on your way home? Thanks.\"\n",
        "]\n",
        "\n",
        "for text in text_examples:\n",
        "    result = classify_text(text, model, tokenizer, device, max_length=train_dataset.max_length)\n",
        "    print(f\"Text: {text}\\nClassification: {result}\\n\")"
      ],
      "metadata": {
        "id": "g4xTwrCtwOUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model for later use\n",
        "torch.save(model.state_dict(), \"smollm2_spam_classifier.pth\")\n",
        "\n",
        "print(\"Model saved successfully! You can reload it with:\")\n",
        "print(\"\"\"\n",
        "# Load the saved model\n",
        "model = SmolLM2Classifier(base_model)  # Initialize the model structure first\n",
        "model.load_state_dict(torch.load(\"smollm2_spam_classifier.pth\"))\n",
        "model.eval()\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "eq_NLLVVwUEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bXWtudXpxg_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise-2"
      ],
      "metadata": {
        "id": "FL2uWOJxbPSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the emotion dataset\n",
        "print(\"Loading emotion dataset...\")\n",
        "dataset = load_dataset(\"emotion\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "Qs42vY0DNzK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataset statistics\n",
        "def get_dataset_stats(dataset):\n",
        "    # Count occurrences of each label in train set\n",
        "    label_counts = {}\n",
        "    for item in dataset[\"train\"]:\n",
        "        label = item[\"label\"]\n",
        "        label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "    # Sort labels by frequency\n",
        "    sorted_labels = sorted(label_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get label names\n",
        "    label_names = dataset[\"train\"].features[\"label\"].names\n",
        "    print(\"Label mapping:\", {i: name for i, name in enumerate(label_names)})\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\nLabel distribution in training set:\")\n",
        "    for label_id, count in sorted_labels:\n",
        "        print(f\"{label_names[label_id]}: {count} samples ({count/len(dataset['train'])*100:.2f}%)\")\n",
        "\n",
        "    return label_names, sorted_labels\n",
        "\n",
        "label_names, sorted_labels = get_dataset_stats(dataset)\n",
        "label_names, sorted_labels"
      ],
      "metadata": {
        "id": "Ft4_1vjHK-bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top 4 emotions for our classification task\n",
        "top_emotions = [label_id for label_id, _ in sorted_labels[:4]]\n",
        "print(f\"\\nWe'll use these 4 emotions for classification: {[label_names[i] for i in top_emotions]}\")"
      ],
      "metadata": {
        "id": "Xy1f6i8sLC0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter dataset to only include the top 4 emotions\n",
        "def filter_top_emotions(example):\n",
        "    return example[\"label\"] in top_emotions\n",
        "\n",
        "filtered_train = dataset[\"train\"].filter(filter_top_emotions)\n",
        "filtered_validation = dataset[\"validation\"].filter(filter_top_emotions)\n",
        "filtered_test = dataset[\"test\"].filter(filter_top_emotions)\n",
        "\n",
        "print(f\"Filtered train set: {len(filtered_train)} samples\")\n",
        "print(f\"Filtered validation set: {len(filtered_validation)} samples\")\n",
        "print(f\"Filtered test set: {len(filtered_test)} samples\")\n",
        "\n",
        "# Create label mapping for the filtered dataset (0-3)\n",
        "emotion_id_mapping = {old_id: new_id for new_id, old_id in enumerate(top_emotions)}\n",
        "\n",
        "# Map old label IDs to new consecutive label IDs\n",
        "def map_labels(example):\n",
        "    example[\"new_label\"] = emotion_id_mapping[example[\"label\"]]\n",
        "    return example\n",
        "\n",
        "filtered_train = filtered_train.map(map_labels)\n",
        "filtered_validation = filtered_validation.map(map_labels)\n",
        "filtered_test = filtered_test.map(map_labels)\n",
        "\n",
        "filtered_train, filtered_test, filtered_validation"
      ],
      "metadata": {
        "id": "OVymgfDMLRNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create balanced dataset\n",
        "def create_balanced_dataset(dataset):\n",
        "    # Find the minimum count among all classes\n",
        "    label_counts = {}\n",
        "    for item in dataset:\n",
        "        label = item[\"new_label\"]\n",
        "        label_counts[label] = label_counts.get(label, 0) + 1\n",
        "\n",
        "    min_count = min(label_counts.values())\n",
        "\n",
        "    # Group examples by label\n",
        "    grouped_examples = {label: [] for label in label_counts.keys()}\n",
        "    for item in dataset:\n",
        "        grouped_examples[item[\"new_label\"]].append(item)\n",
        "\n",
        "    # Sample min_count examples from each class\n",
        "    balanced_examples = []\n",
        "    for label, examples in grouped_examples.items():\n",
        "        sampled_examples = examples[:min_count]  # Take first min_count examples\n",
        "        balanced_examples.extend(sampled_examples)\n",
        "\n",
        "    # Convert to pandas DataFrame\n",
        "    balanced_df = pd.DataFrame({\n",
        "        \"Text\": [item[\"text\"] for item in balanced_examples],\n",
        "        \"Label\": [item[\"new_label\"] for item in balanced_examples]\n",
        "    })\n",
        "\n",
        "    return balanced_df\n",
        "\n",
        "# Create balanced datasets\n",
        "balanced_train_df = create_balanced_dataset(filtered_train)\n",
        "balanced_val_df = create_balanced_dataset(filtered_validation)\n",
        "balanced_test_df = create_balanced_dataset(filtered_test)\n",
        "\n",
        "print(f\"Balanced train set: {len(balanced_train_df)} samples\")\n",
        "print(f\"Balanced validation set: {len(balanced_val_df)} samples\")\n",
        "print(f\"Balanced test set: {len(balanced_test_df)} samples\")\n",
        "\n",
        "# Display class distribution in balanced train set\n",
        "print(balanced_train_df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "id": "NwghxLVkLY9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the balanced datasets\n",
        "balanced_train_df.to_csv(\"train.csv\", index=None)\n",
        "balanced_val_df.to_csv(\"validation.csv\", index=None)\n",
        "balanced_test_df.to_csv(\"test.csv\", index=None)\n",
        "\n",
        "# Get emotion names for the new label mapping\n",
        "new_label_names = [label_names[old_id] for old_id in top_emotions]\n",
        "print(f\"New label mapping: {dict(enumerate(new_label_names))}\")"
      ],
      "metadata": {
        "id": "p1IAjg62LkgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the SmolLM2 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
        "\n",
        "# If tokenizer doesn't have a padding token set, use the EOS token as padding token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_length=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = [\n",
        "            tokenizer.encode(text, add_special_tokens=True)\n",
        "            for text in self.data[\"Text\"]\n",
        "        ]\n",
        "\n",
        "        if max_length is None:\n",
        "            self.max_length = self._longest_encoded_length()\n",
        "        else:\n",
        "            self.max_length = max_length\n",
        "            # Truncate sequences if they are longer than max_length\n",
        "            self.encoded_texts = [\n",
        "                encoded_text[:self.max_length]\n",
        "                for encoded_text in self.encoded_texts\n",
        "            ]\n",
        "\n",
        "        # Pad sequences to the longest sequence\n",
        "        self.encoded_texts = [\n",
        "            encoded_text + [tokenizer.pad_token_id] * (self.max_length - len(encoded_text))\n",
        "            for encoded_text in self.encoded_texts\n",
        "        ]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        encoded = self.encoded_texts[index]\n",
        "        label = self.data.iloc[index][\"Label\"]\n",
        "        return (\n",
        "            torch.tensor(encoded, dtype=torch.long),\n",
        "            torch.tensor(label, dtype=torch.long)\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _longest_encoded_length(self):\n",
        "        max_length = 0\n",
        "        for encoded_text in self.encoded_texts:\n",
        "            encoded_length = len(encoded_text)\n",
        "            if encoded_length > max_length:\n",
        "                max_length = encoded_length\n",
        "        return max_length\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = EmotionDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "print(f\"Maximum sequence length in training set: {train_dataset.max_length}\")\n",
        "\n",
        "val_dataset = EmotionDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "test_dataset = EmotionDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 16  # Increased batch size since emotion texts are typically shorter\n",
        "num_workers = 0\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")\n",
        "\n",
        "# Verify dataset and loader dimensions\n",
        "print(\"Train loader:\")\n",
        "for input_batch, target_batch in train_loader:\n",
        "    break\n",
        "\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "print(\"Label batch dimensions:\", target_batch.shape)\n",
        "\n",
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ],
      "metadata": {
        "id": "2igslkSYLr8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SmolLM2-135M model\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Create a model class that wraps the pretrained model and adds a classification head\n",
        "class SmolLM2Classifier(torch.nn.Module):\n",
        "    def __init__(self, base_model, num_classes=4):  # Changed to 4 classes for emotion\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.config = base_model.config\n",
        "\n",
        "        # Freeze all parameters\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Get embedding dimension\n",
        "        self.hidden_size = self.config.hidden_size\n",
        "\n",
        "        # Add a classification head\n",
        "        self.classifier = torch.nn.Linear(self.hidden_size, num_classes)\n",
        "\n",
        "        # Unfreeze the last transformer block and layer norm\n",
        "        for param in self.base_model.model.layers[-1].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        for param in self.base_model.model.norm.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # Get the output from the base model\n",
        "        outputs = self.base_model(input_ids, output_hidden_states=True)\n",
        "\n",
        "        # Get the last hidden state\n",
        "        last_hidden_state = outputs.hidden_states[-1]\n",
        "\n",
        "        # Apply the classifier to get logits\n",
        "        logits = self.classifier(last_hidden_state)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Initialize the classification model\n",
        "model = SmolLM2Classifier(base_model, num_classes=4)  # 4 emotion classes\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "9Z1ywS9zMGyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    correct_predictions, num_examples = 0, 0\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            num_examples += predicted_labels.shape[0]\n",
        "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return correct_predictions / num_examples\n",
        "\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)[:, -1, :]  # Logits of last output token\n",
        "    loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "    return loss\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "iFXB1v8-MNRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check initial performance\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "    test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(f\"Initial training loss: {train_loss:.3f}\")\n",
        "print(f\"Initial validation loss: {val_loss:.3f}\")\n",
        "print(f\"Initial test loss: {test_loss:.3f}\")\n",
        "\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
        "\n",
        "print(f\"Initial training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Initial validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Initial test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "rUYTJOxwMamc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                            eval_freq, eval_iter):\n",
        "    # Initialize lists to track losses and examples seen\n",
        "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "    examples_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            examples_seen += input_batch.shape[0] # Track examples instead of tokens\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Calculate accuracy after each epoch\n",
        "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "        train_accs.append(train_accuracy)\n",
        "        val_accs.append(val_accuracy)\n",
        "\n",
        "    return train_losses, val_losses, train_accs, val_accs, examples_seen"
      ],
      "metadata": {
        "id": "D9e0592GMiAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Create optimizer - note we're only optimizing the classifier parameters and unfrozen layers\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=5e-5,\n",
        "    weight_decay=0.1\n",
        ")\n",
        "\n",
        "num_epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "Pgz5ZJhCMo7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(label.capitalize())\n",
        "    ax1.legend()\n",
        "\n",
        "    # Create a second x-axis for examples seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(examples_seen, train_values, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(f\"smollm2_emotion_{label}-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
        "\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
      ],
      "metadata": {
        "id": "y4oBLOAPM0HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate final performance on full datasets\n",
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
        "\n",
        "print(f\"Final training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Final validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Final test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "Cs1HyknsM3Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def create_confusion_matrix(model, data_loader, device, class_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_batch, target_batch in data_loader:\n",
        "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "            logits = model(input_batch)[:, -1, :]\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            all_preds.extend(predicted_labels.cpu().numpy())\n",
        "            all_labels.extend(target_batch.cpu().numpy())\n",
        "\n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig(\"emotion_confusion_matrix.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "    return cm\n",
        "\n",
        "# Create confusion matrix for test set\n",
        "cm = create_confusion_matrix(model, test_loader, device, new_label_names)"
      ],
      "metadata": {
        "id": "ObKvutruM94h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_emotion(text, model, tokenizer, device, max_length=None, emotion_names=None):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "\n",
        "    # Truncate if needed\n",
        "    if max_length is not None:\n",
        "        input_ids = input_ids[:max_length]\n",
        "\n",
        "    # Pad if needed\n",
        "    if max_length is not None and len(input_ids) < max_length:\n",
        "        input_ids += [tokenizer.pad_token_id] * (max_length - len(input_ids))\n",
        "\n",
        "    # Convert to tensor and add batch dimension\n",
        "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
        "\n",
        "    # Model inference\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_tensor)[:, -1, :]  # Logits of the last output token\n",
        "\n",
        "    # Get probabilities\n",
        "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    # Return the predicted emotion and confidence\n",
        "    emotion = emotion_names[predicted_label] if emotion_names else str(predicted_label)\n",
        "    confidence = probs[0][predicted_label].item()\n",
        "\n",
        "    return emotion, confidence\n",
        "\n",
        "# Test the classifier with example messages\n",
        "text_examples = [\n",
        "    \"I feel so happy today, everything is going well!\",\n",
        "    \"I can't believe they would do this to me, I'm so angry right now.\",\n",
        "    \"My heart is broken, I can't stop crying after what happened.\",\n",
        "    \"I love spending time with my family, they bring me so much joy.\",\n",
        "    \"That horror movie was so scary, I couldn't sleep all night.\",\n",
        "    \"I miss you so much, can't wait to see you again.\"\n",
        "]\n",
        "\n",
        "print(\"Emotion Classification Results:\")\n",
        "print(\"-\" * 60)\n",
        "for text in text_examples:\n",
        "    emotion, confidence = classify_emotion(text, model, tokenizer, device,\n",
        "                                          max_length=train_dataset.max_length,\n",
        "                                          emotion_names=new_label_names)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Predicted emotion: {emotion} (confidence: {confidence:.2f})\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "2TCeMcgwJKpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model for later use\n",
        "torch.save(model.state_dict(), \"smollm2_emotion_classifier.pth\")\n",
        "\n",
        "print(\"Model saved successfully! You can reload it with:\")\n",
        "print(\"\"\"\n",
        "# Load the saved model\n",
        "model = SmolLM2Classifier(base_model, num_classes=4)  # Initialize the model structure first\n",
        "model.load_state_dict(torch.load(\"smollm2_emotion_classifier.pth\"))\n",
        "model.eval()\n",
        "\"\"\")\n",
        "\n",
        "# Print final information about the dataset and model\n",
        "print(\"\\nFinal model summary:\")\n",
        "print(f\"Dataset: Emotion classification (subset of 4 emotions)\")\n",
        "print(f\"Classes: {new_label_names}\")\n",
        "print(f\"Model: SmolLM2-135M with classification head\")\n",
        "print(f\"Final test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "raHwTjDPNUIJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}